{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "48a84aaa-7c38-4758-9cf6-d92f73b48a26",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Importing libraries...\n"
     ]
    }
   ],
   "source": [
    "print(\"Importing libraries...\")\n",
    "from Bio import SeqIO\n",
    "from pysam import VariantFile\n",
    "import gc\n",
    "import numpy as np\n",
    "\n",
    "import torch\n",
    "from torch import nn\n",
    "from torch.utils.data import Dataset,DataLoader\n",
    "\n",
    "from tqdm import tqdm\n",
    "import sys\n",
    "import time\n",
    "import os\n",
    "from pathlib import Path\n",
    "import pandas as pd\n",
    "from transformers import AutoTokenizer, AutoModelForMaskedLM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "a362b128-4f03-4101-90b7-7465b7a82507",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Data:\n",
    "    def __init__(self, patient):\n",
    "        \"\"\"\n",
    "        Parameters:\n",
    "            self.bed: A .bed file containing the regions with variants.\n",
    "            self.hg38: A human reference genome.\n",
    "            self.patient: The name of the patient (must match the folder containing the .vcf file and the outputs folder).\n",
    "            self.vcf: A .vcf file containing all variants in the patient's genome that have passed the filtering step.\n",
    "        \"\"\"\n",
    "        self.bed = pd.read_csv(f'{patient}/NT_inputs/{patient}_intervals.bed', sep=\"\\t\", header=None)\n",
    "        self.bed.columns = [\"chr\",\"start\",\"end\"]\n",
    "        self.hg38 = SeqIO.to_dict(SeqIO.parse('supporting_files/hg38/hg38.fa','fasta'))\n",
    "        self.patient = patient\n",
    "        self.vcf = VariantFile(f\"{patient}/NT_inputs/{patient}_analysis-ready-variants-combined-sorted.vcf.gz\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "7f55c8ae-ef61-4333-bc9a-0e2f36b5d8f8",
   "metadata": {},
   "outputs": [],
   "source": [
    "class VariantAnalyzer:\n",
    "    def __init__(self, Data, model_name=\"v2-500m-multi-species\", window_length=\"1500\"):\n",
    "        \"\"\"\n",
    "        Parameters:\n",
    "            self.window_length: The amount of bases to the left and right of each variant that is analyzed. This is also the size of the input sequences for the Nucleotide Transformer.\n",
    "            self.model_name: The name of the Nucleotide Transformer model used.\n",
    "            self.model: The downloaded Nucleotide Transformer.\n",
    "            self.tokenizer: The downloaded tokenizer that transforms the sequences into numeric tokens.\n",
    "        \"\"\"\n",
    "        \n",
    "        self.window_length = 2*int(window_length)-2*int(window_length) % 6\n",
    "        self.model_name = model_name\n",
    "        self.model = AutoModelForMaskedLM.from_pretrained(\"InstaDeepAI/nucleotide-transformer-\" + self.model_name, trust_remote_code=True).to(device=\"cuda\")\n",
    "        self.tokenizer = AutoTokenizer.from_pretrained(\"InstaDeepAI/nucleotide-transformer-\" + self.model_name, trust_remote_code=True)\n",
    "        \n",
    "        self.patient = Data.patient\n",
    "        self.hg38 = Data.hg38\n",
    "        self.bed = Data.bed\n",
    "        self.vcf = Data.vcf\n",
    "    def generate_results(self): \n",
    "        \"\"\"\n",
    "        The main function that is used to generate the reference and variant sequences from the regions, tokenize them,\n",
    "        run them through the model, generate the scores, and save those scores.\n",
    "        \"\"\"\n",
    "        \n",
    "        \"\"\"\n",
    "        Checks the outputs folder for already existing files so that chromosomes are not analyzed multiple times.\n",
    "        \"\"\"\n",
    "        chr_filelist = [str(path).split(\"/\")[-1][:-4] for path in Path(f\"{self.patient}/outputs/scores_per_chrom\").glob(\"*\")]\n",
    "        all_chrs = self.bed[\"chr\"].unique()\n",
    "        chrs_to_use = [chrom for chrom in all_chrs if chrom not in chr_filelist]\n",
    "        if len(chrs_to_use) == 0:\n",
    "            print(\"No chromosomes to analyze.\")\n",
    "            return None\n",
    "\n",
    "        \"\"\"\n",
    "        Using binary search, the maximal batch size (the number of window_length long parts of each region that can be run through the\n",
    "        Nucleotide Transformer at once without causing out of memory errors) is determined.\n",
    "        \"\"\"\n",
    "        mem_test_split = np.full((128), [\"A\"*self.window_length])\n",
    "        too_large = True\n",
    "        mem_max = 128\n",
    "        mem_min = 0\n",
    "        while too_large:\n",
    "            mem = int((mem_max+mem_min)/2)\n",
    "            try:\n",
    "                test_embeddings = self.generate_embeddings(mem_test_split[:mem])\n",
    "                if mem_max-mem == 1:\n",
    "                    too_large = False\n",
    "                    self.mem_max = mem\n",
    "                else:\n",
    "                    mem_min = mem\n",
    "            except torch.cuda.OutOfMemoryError:\n",
    "                mem_max = mem\n",
    "                gc.collect()\n",
    "                torch.cuda.empty_cache()\n",
    "                if mem_max == 0:\n",
    "                    raise torch.cuda.OutOfMemoryError    \n",
    "        print(f\"GPUs: {torch.cuda.device_count()}, maximum batch size: {self.mem_max}\")\n",
    "        \n",
    "        for chrom in chrs_to_use:\n",
    "            print(f\"Currently at chromosome {chrom}...\")\n",
    "            temp_indices = list(self.bed[self.bed[\"chr\"]==chrom].index)\n",
    "            unused_indices = []\n",
    "            used_indices = []\n",
    "            ref_seqs = []\n",
    "            var_seqs = []\n",
    "            for index in tqdm(temp_indices):\n",
    "                try:\n",
    "                    row = self.bed.iloc[index]\n",
    "                    var_seq = \"\"\n",
    "                    current = max(0,row[\"start\"]-1)\n",
    "                    var_lag = 0\n",
    "                    for vcf_rec in self.vcf.fetch(row[\"chr\"],row[\"start\"], row[\"end\"]):\n",
    "                        if \"*\" in vcf_rec.ref or \"*\" in vcf_rec.alts[0]:\n",
    "                            continue\n",
    "                        var_seq = var_seq + str(self.hg38[row[\"chr\"]][current:vcf_rec.pos-1].seq).upper() + vcf_rec.alts[0]\n",
    "                        current = vcf_rec.pos+len(vcf_rec.ref)-1\n",
    "                        var_lag = var_lag+len(vcf_rec.ref)-len(vcf_rec.alts[0]) \n",
    "                    ref_seq = str(self.hg38[row[\"chr\"]][max(0,row[\"start\"]-1):row[\"end\"]-1].seq).upper()\n",
    "                    var_seq = var_seq + str(self.hg38[row[\"chr\"]][current:row[\"end\"]-1+var_lag].seq).upper()\n",
    "                    if len(ref_seq) != len(var_seq) or ref_seq == \"\" or var_seq == \"\" or ref_seq.count(\"N\") > 0 or var_seq.count(\"N\") > 0:\n",
    "                        raise ValueError\n",
    "                    else:\n",
    "                        row = self.bed.iloc[index]\n",
    "                        used_indices.append(index)\n",
    "                        ref_seqs.append(ref_seq)\n",
    "                        var_seqs.append(var_seq)\n",
    "                except ValueError as e:\n",
    "                    unused_indices.append(index) #The segment leads to strings of different sizes, which indicates overlapping variants and is therefore skipped\n",
    "            print(f\"In {chrom}, {len(used_indices)} are processed and {len(unused_indices)} regions are skipped due to overlapping indels or N nucleotides being included.\")\n",
    "            \"\"\"\n",
    "            For each region in each chromosome, the output embeddings and scores are generated,\n",
    "            and scores of the same chromsome are concatenated and written to a file.\n",
    "            \"\"\"\n",
    "            t_start = time.time()\n",
    "            results_region = [self.generate_results_region(used_indices[i],ref_seqs[i],var_seqs[i]) for i in tqdm(range(len(used_indices)))]\n",
    "            print(\"Time needed: {:.3f}s\".format(time.time() - t_start))\n",
    "            try: \n",
    "                results_df_this_chrom = pd.concat(results_region)\n",
    "                results_df_this_chrom.to_csv(f\"{self.patient}/outputs/scores_per_chrom/{chrom}.bed\", header=None, index=None, sep='\\t', mode='w+')\n",
    "            except:\n",
    "                print(\"No applicable regions.\")\n",
    "                open(f\"{self.patient}/outputs/scores_per_chrom/{chrom}.bed\",\"w+\").close()\n",
    "    def generate_results_region(self, index, ref_seq, var_seq):\n",
    "        \"\"\"\n",
    "        For each region, the sequences are split into window_length long sections.\n",
    "        Then, the sections are grouped into batches of maximal size self.mem_max and embeddings and scores for both sequences are generated.\n",
    "        \"\"\"\n",
    "        split_window = np.append(np.arange(0,len(ref_seq),self.window_length),len(ref_seq))\n",
    "        ref_seq_split = np.array([ref_seq[split_window[i]:split_window[i+1]] for i in range(len(split_window)-1)])\n",
    "        var_seq_split = np.array([var_seq[split_window[i]:split_window[i+1]] for i in range(len(split_window)-1)])\n",
    "\n",
    "        if len(ref_seq_split)%self.mem_max == 1:\n",
    "            ref_embeddings = torch.Tensor(self.generate_embeddings([ref_seq_split[0]]))\n",
    "            var_embeddings = torch.Tensor(self.generate_embeddings([var_seq_split[0]]))\n",
    "            k = 1\n",
    "        else:\n",
    "            ref_embeddings = torch.Tensor(self.generate_embeddings(ref_seq_split[0:self.mem_max]))\n",
    "            var_embeddings = torch.Tensor(self.generate_embeddings(var_seq_split[0:self.mem_max]))\n",
    "            k = self.mem_max        \n",
    "        cos, dot, l1, mse = self.generate_scores(ref_embeddings,var_embeddings)\n",
    "        for i in range(k, len(ref_seq_split),self.mem_max):\n",
    "            ref_embeddings = torch.Tensor(self.generate_embeddings(ref_seq_split[i:i+self.mem_max]))\n",
    "            var_embeddings = torch.Tensor(self.generate_embeddings(var_seq_split[i:i+self.mem_max]))\n",
    "            this_cos, this_dot,this_l1, this_mse = self.generate_scores(ref_embeddings,var_embeddings)\n",
    "            cos = torch.cat((cos, this_cos))\n",
    "            l1 = torch.cat((l1, this_l1))\n",
    "            mse = torch.cat((mse, this_mse))\n",
    "            dot = torch.cat((dot,this_dot)) \n",
    "        \"\"\"\n",
    "        The dataframe containing information about the current region is created and returned.\n",
    "        \"\"\"\n",
    "        pos_6mer = np.arange(self.bed.iloc[index][\"start\"],self.bed.iloc[index][\"start\"]+len(ref_seq),6,dtype=np.int64)\n",
    "        index_of_6mer = np.full(fill_value=index,shape=len(pos_6mer))\n",
    "        chr_6mer = np.full(fill_value=self.bed.iloc[index][\"chr\"],shape=len(pos_6mer))\n",
    "        cos = cos[:len(pos_6mer)]\n",
    "        l1 = l1[:len(pos_6mer)]\n",
    "        mse = mse[:len(pos_6mer)] \n",
    "        dot = dot[:len(pos_6mer)]\n",
    "        this_df =  pd.DataFrame({\"Chromosome\":chr_6mer,\"Index_in_bed\":index_of_6mer,\"Begin_6mer\":pos_6mer,\"Cosine_Similarity\":cos,\"Dot_Product\":dot,\"1-L1_Loss\":l1,\"1-MSE_Loss\":mse})\n",
    "        return this_df\n",
    "    def generate_embeddings(self, split):     \n",
    "        \"\"\"\n",
    "        The embeddings of the current batch/split are calculated. Then, occupied memory is freed.\n",
    "        \"\"\"\n",
    "        gc.collect()\n",
    "        torch.cuda.empty_cache()\n",
    "        \n",
    "        tokens_ids = self.tokenizer.batch_encode_plus(split, return_tensors=\"pt\",padding=\"longest\")[\"input_ids\"].to(device=\"cuda\")\n",
    "        attention_mask = tokens_ids != self.tokenizer.pad_token_id\n",
    "        attention_mask.to(\"cuda\")\n",
    "        \n",
    "        torch_outs = self.model(\n",
    "            tokens_ids,\n",
    "            attention_mask=attention_mask,\n",
    "            encoder_attention_mask=attention_mask,\n",
    "            output_hidden_states=True\n",
    "        )\n",
    "        \n",
    "        embeddings = torch_outs['hidden_states'][-1].detach()\n",
    "        attention_mask = torch.unsqueeze(attention_mask, dim=-1)\n",
    "        embeddings = (attention_mask * embeddings)[:,1:,:]\n",
    "        embeddings = embeddings.cpu()\n",
    "        gc.collect()\n",
    "        torch.cuda.empty_cache()\n",
    "        return embeddings\n",
    "    def generate_scores(self, ref_embeddings, var_embeddings):\n",
    "        \"\"\"\n",
    "        The cosine similarity, dot product, manhattan distance and euclidian distance are calculated\n",
    "        between the embeddings of the variant and reference of the same region.\n",
    "        \"\"\"\n",
    "        cos_metric = nn.CosineSimilarity(dim=-1)\n",
    "        l1_metric = nn.L1Loss()\n",
    "        mse_metric = nn.MSELoss() \n",
    "        \n",
    "        ref_embeddings = torch.cat(tuple(ref_embeddings[i,:,:] for i in range(ref_embeddings.shape[0])),axis=0)\n",
    "        var_embeddings = torch.cat(tuple(var_embeddings[i,:,:] for i in range(var_embeddings.shape[0])),axis=0)\n",
    "        \n",
    "        cos = cos_metric(ref_embeddings,var_embeddings)\n",
    "        l1 = torch.tensor([1-l1_metric(ref_embeddings[i,:],var_embeddings[i,:]).item() for i in range(ref_embeddings.shape[0])])\n",
    "        mse = torch.tensor([1-mse_metric(ref_embeddings[i,:],var_embeddings[i,:]).item() for i in range(ref_embeddings.shape[0])])\n",
    "        dot = torch.tensor([torch.dot(ref_embeddings[i,:],var_embeddings[i,:]).item() for i in range(ref_embeddings.shape[0])])\n",
    "        return (cos,dot,l1,mse)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "60768cc8-d047-4e78-9693-8240b9af6fdb",
   "metadata": {},
   "outputs": [],
   "source": [
    "patient = \"CHG034730\"\n",
    "window_length = 1500\n",
    "models = [\"2.5b-multi-species\",\"2.5b-1000g\",\"500m-human-ref\",\"500m-1000g\",\"v2-50m-multi-species\",\"v2-100m-multi-species\",\"v2-250m-multi-species\",\"v2-500m-multi-species\"] #Model can be specified by initializing nt_class with model_name = models[i]\n",
    "model_name = models[-1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "d88be2df-5468-4dde-82b3-d5a6e27edd47",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_class = Data(patient)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "420e131c-45d6-4990-8f4b-9e136a2ae659",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading Data...\n"
     ]
    },
    {
     "ename": "AssertionError",
     "evalue": "Torch not compiled with CUDA enabled",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAssertionError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[19], line 2\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mLoading Data...\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m----> 2\u001b[0m nt_class \u001b[38;5;241m=\u001b[39m \u001b[43mVariantAnalyzer\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdata_class\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmodel_name\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mmodel_name\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mwindow_length\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mwindow_length\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m      3\u001b[0m nt_class\u001b[38;5;241m.\u001b[39mgenerate_results()\n",
      "Cell \u001b[0;32mIn[16], line 13\u001b[0m, in \u001b[0;36mVariantAnalyzer.__init__\u001b[0;34m(self, Data, model_name, window_length)\u001b[0m\n\u001b[1;32m     11\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mwindow_length \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m2\u001b[39m\u001b[38;5;241m*\u001b[39m\u001b[38;5;28mint\u001b[39m(window_length)\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m2\u001b[39m\u001b[38;5;241m*\u001b[39m\u001b[38;5;28mint\u001b[39m(window_length) \u001b[38;5;241m%\u001b[39m \u001b[38;5;241m6\u001b[39m\n\u001b[1;32m     12\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmodel_name \u001b[38;5;241m=\u001b[39m model_name\n\u001b[0;32m---> 13\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmodel \u001b[38;5;241m=\u001b[39m \u001b[43mAutoModelForMaskedLM\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfrom_pretrained\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mInstaDeepAI/nucleotide-transformer-\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;241;43m+\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmodel_name\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtrust_remote_code\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mto\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdevice\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mcuda\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m     14\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtokenizer \u001b[38;5;241m=\u001b[39m AutoTokenizer\u001b[38;5;241m.\u001b[39mfrom_pretrained(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mInstaDeepAI/nucleotide-transformer-\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;241m+\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmodel_name, trust_remote_code\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[1;32m     16\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpatient \u001b[38;5;241m=\u001b[39m Data\u001b[38;5;241m.\u001b[39mpatient\n",
      "File \u001b[0;32m/fast/AG_Metzger/georg/miniforge3/envs/wgt2/lib/python3.12/site-packages/transformers/modeling_utils.py:2883\u001b[0m, in \u001b[0;36mPreTrainedModel.to\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   2878\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m dtype_present_in_args:\n\u001b[1;32m   2879\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[1;32m   2880\u001b[0m             \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mYou cannot cast a GPTQ model in a new `dtype`. Make sure to load the model using `from_pretrained` using the desired\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m   2881\u001b[0m             \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m `dtype` by passing the correct `torch_dtype` argument.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m   2882\u001b[0m         )\n\u001b[0;32m-> 2883\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43msuper\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mto\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/fast/AG_Metzger/georg/miniforge3/envs/wgt2/lib/python3.12/site-packages/torch/nn/modules/module.py:1173\u001b[0m, in \u001b[0;36mModule.to\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1170\u001b[0m         \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m   1171\u001b[0m             \u001b[38;5;28;01mraise\u001b[39;00m\n\u001b[0;32m-> 1173\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_apply\u001b[49m\u001b[43m(\u001b[49m\u001b[43mconvert\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/fast/AG_Metzger/georg/miniforge3/envs/wgt2/lib/python3.12/site-packages/torch/nn/modules/module.py:779\u001b[0m, in \u001b[0;36mModule._apply\u001b[0;34m(self, fn, recurse)\u001b[0m\n\u001b[1;32m    777\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m recurse:\n\u001b[1;32m    778\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m module \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mchildren():\n\u001b[0;32m--> 779\u001b[0m         \u001b[43mmodule\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_apply\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfn\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    781\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mcompute_should_use_set_data\u001b[39m(tensor, tensor_applied):\n\u001b[1;32m    782\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m torch\u001b[38;5;241m.\u001b[39m_has_compatible_shallow_copy_type(tensor, tensor_applied):\n\u001b[1;32m    783\u001b[0m         \u001b[38;5;66;03m# If the new tensor has compatible tensor type as the existing tensor,\u001b[39;00m\n\u001b[1;32m    784\u001b[0m         \u001b[38;5;66;03m# the current behavior is to change the tensor in-place using `.data =`,\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    789\u001b[0m         \u001b[38;5;66;03m# global flag to let the user control whether they want the future\u001b[39;00m\n\u001b[1;32m    790\u001b[0m         \u001b[38;5;66;03m# behavior of overwriting the existing tensor or not.\u001b[39;00m\n",
      "File \u001b[0;32m/fast/AG_Metzger/georg/miniforge3/envs/wgt2/lib/python3.12/site-packages/torch/nn/modules/module.py:779\u001b[0m, in \u001b[0;36mModule._apply\u001b[0;34m(self, fn, recurse)\u001b[0m\n\u001b[1;32m    777\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m recurse:\n\u001b[1;32m    778\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m module \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mchildren():\n\u001b[0;32m--> 779\u001b[0m         \u001b[43mmodule\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_apply\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfn\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    781\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mcompute_should_use_set_data\u001b[39m(tensor, tensor_applied):\n\u001b[1;32m    782\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m torch\u001b[38;5;241m.\u001b[39m_has_compatible_shallow_copy_type(tensor, tensor_applied):\n\u001b[1;32m    783\u001b[0m         \u001b[38;5;66;03m# If the new tensor has compatible tensor type as the existing tensor,\u001b[39;00m\n\u001b[1;32m    784\u001b[0m         \u001b[38;5;66;03m# the current behavior is to change the tensor in-place using `.data =`,\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    789\u001b[0m         \u001b[38;5;66;03m# global flag to let the user control whether they want the future\u001b[39;00m\n\u001b[1;32m    790\u001b[0m         \u001b[38;5;66;03m# behavior of overwriting the existing tensor or not.\u001b[39;00m\n",
      "File \u001b[0;32m/fast/AG_Metzger/georg/miniforge3/envs/wgt2/lib/python3.12/site-packages/torch/nn/modules/module.py:779\u001b[0m, in \u001b[0;36mModule._apply\u001b[0;34m(self, fn, recurse)\u001b[0m\n\u001b[1;32m    777\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m recurse:\n\u001b[1;32m    778\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m module \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mchildren():\n\u001b[0;32m--> 779\u001b[0m         \u001b[43mmodule\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_apply\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfn\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    781\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mcompute_should_use_set_data\u001b[39m(tensor, tensor_applied):\n\u001b[1;32m    782\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m torch\u001b[38;5;241m.\u001b[39m_has_compatible_shallow_copy_type(tensor, tensor_applied):\n\u001b[1;32m    783\u001b[0m         \u001b[38;5;66;03m# If the new tensor has compatible tensor type as the existing tensor,\u001b[39;00m\n\u001b[1;32m    784\u001b[0m         \u001b[38;5;66;03m# the current behavior is to change the tensor in-place using `.data =`,\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    789\u001b[0m         \u001b[38;5;66;03m# global flag to let the user control whether they want the future\u001b[39;00m\n\u001b[1;32m    790\u001b[0m         \u001b[38;5;66;03m# behavior of overwriting the existing tensor or not.\u001b[39;00m\n",
      "File \u001b[0;32m/fast/AG_Metzger/georg/miniforge3/envs/wgt2/lib/python3.12/site-packages/torch/nn/modules/module.py:804\u001b[0m, in \u001b[0;36mModule._apply\u001b[0;34m(self, fn, recurse)\u001b[0m\n\u001b[1;32m    800\u001b[0m \u001b[38;5;66;03m# Tensors stored in modules are graph leaves, and we don't want to\u001b[39;00m\n\u001b[1;32m    801\u001b[0m \u001b[38;5;66;03m# track autograd history of `param_applied`, so we have to use\u001b[39;00m\n\u001b[1;32m    802\u001b[0m \u001b[38;5;66;03m# `with torch.no_grad():`\u001b[39;00m\n\u001b[1;32m    803\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m torch\u001b[38;5;241m.\u001b[39mno_grad():\n\u001b[0;32m--> 804\u001b[0m     param_applied \u001b[38;5;241m=\u001b[39m \u001b[43mfn\u001b[49m\u001b[43m(\u001b[49m\u001b[43mparam\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    805\u001b[0m p_should_use_set_data \u001b[38;5;241m=\u001b[39m compute_should_use_set_data(param, param_applied)\n\u001b[1;32m    807\u001b[0m \u001b[38;5;66;03m# subclasses may have multiple child tensors so we need to use swap_tensors\u001b[39;00m\n",
      "File \u001b[0;32m/fast/AG_Metzger/georg/miniforge3/envs/wgt2/lib/python3.12/site-packages/torch/nn/modules/module.py:1159\u001b[0m, in \u001b[0;36mModule.to.<locals>.convert\u001b[0;34m(t)\u001b[0m\n\u001b[1;32m   1152\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m convert_to_format \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m t\u001b[38;5;241m.\u001b[39mdim() \u001b[38;5;129;01min\u001b[39;00m (\u001b[38;5;241m4\u001b[39m, \u001b[38;5;241m5\u001b[39m):\n\u001b[1;32m   1153\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m t\u001b[38;5;241m.\u001b[39mto(\n\u001b[1;32m   1154\u001b[0m             device,\n\u001b[1;32m   1155\u001b[0m             dtype \u001b[38;5;28;01mif\u001b[39;00m t\u001b[38;5;241m.\u001b[39mis_floating_point() \u001b[38;5;129;01mor\u001b[39;00m t\u001b[38;5;241m.\u001b[39mis_complex() \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[1;32m   1156\u001b[0m             non_blocking,\n\u001b[1;32m   1157\u001b[0m             memory_format\u001b[38;5;241m=\u001b[39mconvert_to_format,\n\u001b[1;32m   1158\u001b[0m         )\n\u001b[0;32m-> 1159\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mt\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mto\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   1160\u001b[0m \u001b[43m        \u001b[49m\u001b[43mdevice\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1161\u001b[0m \u001b[43m        \u001b[49m\u001b[43mdtype\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mif\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mt\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mis_floating_point\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01mor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mt\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mis_complex\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01melse\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m   1162\u001b[0m \u001b[43m        \u001b[49m\u001b[43mnon_blocking\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1163\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1164\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mNotImplementedError\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[1;32m   1165\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mstr\u001b[39m(e) \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mCannot copy out of meta tensor; no data!\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n",
      "File \u001b[0;32m/fast/AG_Metzger/georg/miniforge3/envs/wgt2/lib/python3.12/site-packages/torch/cuda/__init__.py:284\u001b[0m, in \u001b[0;36m_lazy_init\u001b[0;34m()\u001b[0m\n\u001b[1;32m    279\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mRuntimeError\u001b[39;00m(\n\u001b[1;32m    280\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mCannot re-initialize CUDA in forked subprocess. To use CUDA with \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    281\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmultiprocessing, you must use the \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mspawn\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m start method\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    282\u001b[0m     )\n\u001b[1;32m    283\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28mhasattr\u001b[39m(torch\u001b[38;5;241m.\u001b[39m_C, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m_cuda_getDeviceCount\u001b[39m\u001b[38;5;124m\"\u001b[39m):\n\u001b[0;32m--> 284\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mAssertionError\u001b[39;00m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mTorch not compiled with CUDA enabled\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m    285\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m _cudart \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    286\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mAssertionError\u001b[39;00m(\n\u001b[1;32m    287\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mlibcudart functions unavailable. It looks like you have a broken build?\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    288\u001b[0m     )\n",
      "\u001b[0;31mAssertionError\u001b[0m: Torch not compiled with CUDA enabled"
     ]
    }
   ],
   "source": [
    "print(\"Loading Data...\")\n",
    "nt_class = VariantAnalyzer(data_class, model_name = model_name, window_length = window_length)\n",
    "nt_class.generate_results()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
